{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VirajOke/Data-Science-Projects/blob/main/NLP_NLTK_Tokenize_Bigrams.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWZ0MfPTAw2S"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "#%cd drive/My Drive/DIRECTORY_IN_YOUR_DRIVE\n",
        "#files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Download these before suing them\n",
        "### import nltk\n",
        "### nltk.download('punkt')\n",
        "### import nltk\n",
        "### nltk.download('stopwords')\n",
        "```"
      ],
      "metadata": {
        "id": "y4nVrY8qKKDY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used contraction package to deal with contractions such as \n",
        "I've => I have"
      ],
      "metadata": {
        "id": "CylFqHqMKQHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6MICOrLKNwP",
        "outputId": "d64012b5-74c2-431d-fb54-c9faa9e1a50a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.66-py2.py3-none-any.whl (8.0 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting anyascii\n",
            "  Downloading anyascii-0.3.0-py3-none-any.whl (284 kB)\n",
            "\u001b[K     |████████████████████████████████| 284 kB 9.3 MB/s \n",
            "\u001b[?25hCollecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.2.tar.gz (321 kB)\n",
            "\u001b[K     |████████████████████████████████| 321 kB 39.7 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85453 sha256=51fe958141007a856271c6b365a054a23f22917cf367a9279b84eb2371a2911d\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/19/a6/8f363d9939162782bb8439d886469756271abc01f76fbd790f\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.0 contractions-0.1.66 pyahocorasick-1.4.2 textsearch-0.0.21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  nltk.download('punkt')\n",
        "  nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4zInH_PKFr9",
        "outputId": "9c0fce13-7c6d-44ed-9ae5-cdae94162c68"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "bIVefGDlDjTe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import re\n",
        "import contractions\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.probability import FreqDist"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing \n"
      ],
      "metadata": {
        "id": "_TpWRFmU2RKD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "YsI6wOjQDhoW"
      },
      "outputs": [],
      "source": [
        "def read_corpus(path_filename):\n",
        "  text= open(path_filename, encoding='cp1252')\n",
        "  corpus= text.read().replace(\"\\n\",\" \")\n",
        "  text.close()\n",
        "  return corpus"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def token_freq_sort_df(clean_corpus,path_filename):\n",
        "  temp, count= np.unique(clean_corpus, return_counts= True)\n",
        "  token_freq_arr = np.column_stack((temp, count))\n",
        "  token_freq_df = pd.DataFrame(token_freq_arr, columns=['Tokens','Frequency'])\n",
        "  token_freq_df['Frequency']= token_freq_df['Frequency'].astype(int)\n",
        "  token_freq_df.sort_values(by=['Frequency'],axis= 0, ascending= False, \n",
        "                          kind='quicksort', inplace= True)\n",
        "  path= path_filename  \n",
        "  token_freq_df.to_csv(path)\n",
        "  return token_freq_df"
      ],
      "metadata": {
        "id": "IAeLhh1J3JiB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1OZJ2eO4aJkU"
      },
      "outputs": [],
      "source": [
        "def only_words(corpus):\n",
        "  regex1= r'[^A-Za-z]|[0-9]'\n",
        "  regex2= r'[^A-Za-z]'\n",
        "  temp1= re.sub(regex1,' ',corpus)\n",
        "  temp2=re.sub(\"\\s+\",' ',temp1)\n",
        "  corpus2= re.split(regex2, temp2)\n",
        "  return corpus2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def stopwords_func(corpus2):\n",
        "  stopwords1= stopwords.words('english')\n",
        "  clean_corpus= []\n",
        "  for i in corpus2:\n",
        "    if i not in stopwords1:\n",
        "      clean_corpus.append(i)\n",
        "  return clean_corpus"
      ],
      "metadata": {
        "id": "3rf82Hldb0gm"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ryGeVZQRLVc1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used the regex rule to tokenize each and every word OR number OR symbol.\n",
        "Because if we directly use word.tokenize() it doesnt do well with words used with apostrophe.\n",
        "it wil generate something like (Viraj's => 'Viraj','s'') instead of ('Viraj',' ' ', 's')\n",
        "\n",
        "Corpus without Contraction. Cause I think it would make much more sense \n",
        "if the tokens are like ['I','have'] rather than ['I',''','ve']\n"
      ],
      "metadata": {
        "id": "R8Ckoo8LDvRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file1= '/content/drive/MyDrive/Colab_Notebooks/sample.txt'\n",
        "corpus= read_corpus(file1)\n",
        "corpus= contractions.fix(corpus, slang= True)"
      ],
      "metadata": {
        "id": "BRF472e8DQmL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regex= r'[A-Za-z]+|[,.;:\\'\\\"!@#$%^&*\\(\\)]+|[0-9]+'\n",
        "corpus1= re.findall(regex, corpus)"
      ],
      "metadata": {
        "id": "0rybUSH8FVJV"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path1= \"/content/drive/MyDrive/Colab_Notebooks/tokens.csv\"\n",
        "df1= token_freq_sort_df(corpus1,path1)"
      ],
      "metadata": {
        "id": "LLrVOteDHshw"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus2= only_words(corpus)"
      ],
      "metadata": {
        "id": "yBfu5ZrnE8b-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus3= stopwords_func(corpus2)"
      ],
      "metadata": {
        "id": "ipJPuVeGF3PG"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path2= \"/content/drive/MyDrive/Colab_Notebooks/no_stop_words.csv\"\n",
        "df2= token_freq_sort_df(corpus3,path2)"
      ],
      "metadata": {
        "id": "4Oy1QJSwGtFq"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram = list(nltk.bigrams(corpus3))"
      ],
      "metadata": {
        "id": "Z3LNdT9nJBkT"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path4= \"/content/drive/MyDrive/Colab_Notebooks/only_words_freq.csv\"\n",
        "df4= token_freq_sort_df(corpus2,path4)"
      ],
      "metadata": {
        "id": "-LyHwqXxxPGu"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq_bigrams= FreqDist(bigram)"
      ],
      "metadata": {
        "id": "Fpbk3TV-lhM6"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_tuple= []\n",
        "for i in freq_bigrams.items():\n",
        "    list_of_tuple.append(tuple(i))"
      ],
      "metadata": {
        "id": "f7DkMr5X4vAP"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_tuple\n",
        "df5 = pd.DataFrame(list_of_tuple, columns=['Tokens','Frequency'])\n",
        "df5['Frequency']= df5['Frequency'].astype(int)\n",
        "df5.sort_values(by=['Frequency'],axis= 0, ascending= False, \n",
        "                          kind='quicksort', inplace= True)\n",
        "path= \"/content/drive/MyDrive/Colab_Notebooks/bigram_freq.csv\"  \n",
        "df5.to_csv(path)"
      ],
      "metadata": {
        "id": "sMAmOoKb4u3A"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Results\n",
        "total_tokens= len(corpus1)\n",
        "corpus_arr= np.array(corpus1)\n",
        "unique_tokens= len(np.unique(corpus_arr))\n",
        "token_ratio= unique_tokens/total_tokens\n",
        "token_count, freq_count = df1[df1['Frequency']==1].count()\n",
        "token_count\n",
        "only_words_len= len(corpus2)\n",
        "only_words_ratio= only_words_len/total_tokens\n",
        "stop_words_len= len(corpus3)\n",
        "no_stop_ratio= stop_words_len/total_tokens"
      ],
      "metadata": {
        "id": "mqkTllfG6K4z"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of tokens in the corpus:\", total_tokens)\n",
        "print(\"Number of unique tokens:\", unique_tokens )\n",
        "print(\"Unique to tokens ratio:\",'%2f'%token_ratio )\n",
        "print(\"Count of tokens having (frequence = 1):\",token_count)\n",
        "print(\"Count of only words:\",only_words_len)\n",
        "print(\"Only words to total tokens ratio/lexical diversity:\",'%2f'%only_words_ratio)\n",
        "print(\"No stop words to tokens ratio/lexical density:\",'%2f'%no_stop_ratio)\n"
      ],
      "metadata": {
        "id": "aAGEghzpg2DF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9e60a7d-335a-4e45-e3bf-2ab9eed83e98"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens in the corpus: 221618\n",
            "Number of unique tokens: 22198\n",
            "Unique to tokens ratio: 0.100163\n",
            "Count of tokens having (frequence = 1): 12353\n",
            "Count of only words: 191599\n",
            "Only words to total tokens ratio/lexical diversity: 0.864546\n",
            "No stop words to tokens ratio/lexical density: 0.493525\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The functions defined by us produced exactly similar \n",
        "outputs as the inbuilt functions mentioned below.\n",
        "\n",
        "temp= FreqDist(corpus1)\n",
        "temp.most_common()\n",
        "temp2= FreqDist(corpus3)\n",
        "temp2.most_common()\n",
        "temp3= FreqDist(corpus2)\n",
        "temp3.most_common()\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "e1FEdvDGg65x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "1f84a2b5-c047-44f3-8392-eaf24c5c2ba3"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ntemp= FreqDist(corpus1)\\ntemp.most_common()\\ntemp2= FreqDist(corpus3)\\ntemp2.most_common()\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sgJC14puhvhN"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZWKYLe_xiIo3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "NLP_Assignment_1.ipynb",
      "provenance": [],
      "mount_file_id": "1Entp_x2u3ED6j986wT0DwYRE2v9U9AiG",
      "authorship_tag": "ABX9TyO0oKyMMDbW0srhQdBbnpNX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}